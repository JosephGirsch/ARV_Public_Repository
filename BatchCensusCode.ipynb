{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: This script will obtain additional columns of geo data for each property in the loaded real estate data set. \n",
    "\n",
    "WARNING: This script is expected to take approximately ~2 days to run given the approximately 300,000 rows of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Code: \n",
    "* Import Modules, Constants, and Functions.\n",
    "* Load Raw Data\n",
    "* Step 0: Initialize column changes for prebatch df\n",
    "* Step 1: Initialize df_postBatch with first GeoCode Batch (Currently takes about 20 hours)\n",
    "* (Steps 2-4) Attempt a few 'cleanup' loops. Diminishing returns after 3 or 4 cleanup loops.\n",
    "    * Step 2: Filter only to rows that failed to match. \n",
    "    * Step 3: getTract_batchAddressGeocode only those that failed to match earlier. \n",
    "    * Step 4: Update postbatch with new data.\n",
    "* Step 5: Save the aggregated postgeo file as a single csv (for inspection purposes only).\n",
    "* Step 6: Check that list lengths and indexes of the original dataframe, prebatch dataframe, and postbatch dataframe are all the same.\n",
    "* Step 7: Reverse column name changes and save as MarylandData_postGeo_orig2.csv. \n",
    "* Step 8: Load the previously saved \"_orig2\" file.\n",
    "* Step 9: Identify rows with valid Lat/Long and loop until each row has an accompaning 'tract' value.\n",
    "* Step 10: Save the final CSV and print the insane amount of time the script took to run. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# Import Modules, Constants, and Functions.\n",
    "%run ARV_Functions_Constants.ipynb\n",
    "start_time = time.time()\n",
    "file_path_census = r\"intermediate_BatchCensusCode_files\\\\\"\n",
    "\n",
    "# Census Geocoder Documentation:\n",
    "# https://pypi.org/project/censusgeocode/\n",
    "# https://geocoding.geo.census.gov/geocoder/Geocoding_Services_API.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:50: DtypeWarning: Columns (65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:50: DtypeWarning: Columns (65) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:50: DtypeWarning: Columns (25,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:50: DtypeWarning: Columns (64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:50: DtypeWarning: Columns (64) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "<timed exec>:50: DtypeWarning: Columns (64) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "# Load Raw Data\n",
    "df = read_small_csvs_as_big_df(\"raw_real_estate_files//\", na_keep = False, dtype_dict = {'Levels':'object'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! No Address has \" in it!\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Initialize column changes for prebatch df\n",
    "batchColumns_inputDict = {'FullStreetAddress':'Street address','City':'City','StateOrProvince':'State','PostalCode':'ZIP'}\n",
    "batchColumns_inverseDict = {v: k for k, v in batchColumns_inputDict.items()}\n",
    "outputBatchCols = ['address','match','tract','block']\n",
    "df_preBatch = df.rename(columns = batchColumns_inputDict)\n",
    "df_preBatch['Unique ID'] = df_preBatch.index\n",
    "\n",
    "# Confirm no rows contain the dreaded '\"'\n",
    "if df_preBatch[df_preBatch[\"Street address\"].str.contains('\"',na=False)].empty:\n",
    "    print(\"Correct! No Address has \\\" in it!\")\n",
    "else:\n",
    "    print(\"Problem! Row has \\\" in it. Remove these rows! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting datetime: 2022-07-09 15:44:53.579819\n",
      "The raw data frame of 19980 rows is split into 2 dataframes of 9990 length each (9990 rows in the last df.)\n",
      "Trying Batch: 1... "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 1: Initialize df_postBatch with first GeoCode Batch (Currently takes about 20 hours)\n",
    "df_geo_merge1 = getTract_batchAddressGeocode(df_preBatch) \n",
    "df_postBatch = df_preBatch.join(df_geo_merge1[outputBatchCols], how='left', on='Unique ID',lsuffix='_duplicate1', rsuffix='_duplicate2')\n",
    "print(\"\\n\",df_postBatch.match.value_counts(),sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_postBatch.match.value_counts(): True     313656\n",
      "False     24148\n",
      "Name: match, dtype: int64\n",
      "\n",
      "Does the df_preBatch_loop index and Unique ID still match?: True\n",
      "Starting datetime: 2021-03-05 20:21:44.829393\n",
      "The raw data frame of 24148 rows is split into 3 dataframes of 9990 length each (4168 rows in the last df.)\n",
      "Trying Batch: 1... SUCCESS. Batch: 1 correct.  \tdatetime: 2021-03-05 21:24:31.572172\n",
      "Trying Batch: 2... SUCCESS. Batch: 2 correct.  \tdatetime: 2021-03-05 22:33:48.338373\n",
      "Trying Batch: 3... SUCCESS. Batch: 3 correct.  \tdatetime: 2021-03-05 23:00:11.158025\n",
      "\n",
      "Starting Merge... \tdatetime: 2021-03-05 23:00:11.190014\n",
      "len(_list_geo_dfs): 3\n",
      "Broken batches:  dict_keys([]) \n",
      "\n",
      "Finished Merge.Returning df. \tdatetime: 2021-03-05 23:00:11.237017\n",
      "_df_geo_merge.match.value_counts(): False    21814\n",
      "True      2334\n",
      "Name: match, dtype: int64\n",
      "\n",
      "True     315990\n",
      "False     21814\n",
      "Name: match, dtype: int64\n",
      "\n",
      "Does the df_preBatch_loop index and Unique ID still match?: True\n",
      "Starting datetime: 2021-03-05 23:00:15.406816\n",
      "The raw data frame of 21814 rows is split into 3 dataframes of 9990 length each (1834 rows in the last df.)\n",
      "Trying Batch: 1... SUCCESS. Batch: 1 correct.  \tdatetime: 2021-03-05 23:39:41.182170\n",
      "Trying Batch: 2... SUCCESS. Batch: 2 correct.  \tdatetime: 2021-03-06 00:14:14.671677\n",
      "Trying Batch: 3... SUCCESS. Batch: 3 correct.  \tdatetime: 2021-03-06 00:18:27.117726\n",
      "\n",
      "Starting Merge... \tdatetime: 2021-03-06 00:18:27.139953\n",
      "len(_list_geo_dfs): 3\n",
      "Broken batches:  dict_keys([]) \n",
      "\n",
      "Finished Merge.Returning df. \tdatetime: 2021-03-06 00:18:27.172997\n",
      "_df_geo_merge.match.value_counts(): False    19839\n",
      "True      1975\n",
      "Name: match, dtype: int64\n",
      "\n",
      "True     317965\n",
      "False     19839\n",
      "Name: match, dtype: int64\n",
      "\n",
      "Does the df_preBatch_loop index and Unique ID still match?: True\n",
      "Starting datetime: 2021-03-06 00:18:31.250778\n",
      "The raw data frame of 19839 rows is split into 2 dataframes of 9990 length each (9849 rows in the last df.)\n",
      "Trying Batch: 1... SUCCESS. Batch: 1 correct.  \tdatetime: 2021-03-06 00:52:48.725164\n",
      "Trying Batch: 2... SUCCESS. Batch: 2 correct.  \tdatetime: 2021-03-06 01:30:47.157656\n",
      "\n",
      "Starting Merge... \tdatetime: 2021-03-06 01:30:47.233421\n",
      "len(_list_geo_dfs): 2\n",
      "Broken batches:  dict_keys([]) \n",
      "\n",
      "Finished Merge.Returning df. \tdatetime: 2021-03-06 01:30:47.265471\n",
      "_df_geo_merge.match.value_counts(): False    18886\n",
      "True       953\n",
      "Name: match, dtype: int64\n",
      "\n",
      "True     318918\n",
      "False     18886\n",
      "Name: match, dtype: int64\n",
      "\n",
      "Does the df_preBatch_loop index and Unique ID still match?: True\n",
      "Starting datetime: 2021-03-06 01:30:51.218744\n",
      "The raw data frame of 18886 rows is split into 2 dataframes of 9990 length each (8896 rows in the last df.)\n",
      "Trying Batch: 1... SUCCESS. Batch: 1 correct.  \tdatetime: 2021-03-06 02:10:25.805486\n",
      "Trying Batch: 2... SUCCESS. Batch: 2 correct.  \tdatetime: 2021-03-06 02:30:21.769066\n",
      "\n",
      "Starting Merge... \tdatetime: 2021-03-06 02:30:21.836074\n",
      "len(_list_geo_dfs): 2\n",
      "Broken batches:  dict_keys([]) \n",
      "\n",
      "Finished Merge.Returning df. \tdatetime: 2021-03-06 02:30:21.868157\n",
      "_df_geo_merge.match.value_counts(): False    17234\n",
      "True      1652\n",
      "Name: match, dtype: int64\n",
      "\n",
      "True     320570\n",
      "False     17234\n",
      "Name: match, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (Steps 2-4) Attempt a few 'cleanup' loops. Diminishing returns after 3 or 4 cleanup loops. \n",
    "print(\"df_postBatch.match.value_counts(): \",df_postBatch.match.value_counts(),\"\\n\",sep=\"\")\n",
    "n=4\n",
    "for i in range(n):\n",
    "    if 'False' in df_postBatch.match.unique():\n",
    "        # Step 2: Filter only to rows that failed to match. \n",
    "        df_preBatch_loop = df_postBatch.loc[df_postBatch.tract.isna(), :].copy(); print(\"Does the df_preBatch_loop index and Unique ID still match?:\",(df_preBatch_loop['Unique ID'].values == df_preBatch_loop.index.values).all())\n",
    "\n",
    "        # Step 3: getTract_batchAddressGeocode only those that failed to match earlier. \n",
    "        df_geo_merge_loop = getTract_batchAddressGeocode(df_preBatch_loop, cleanupLoop = True) \n",
    "\n",
    "        # Step 4: Update postbatch with new data.\n",
    "        df_postBatch.update(df_geo_merge_loop.loc[:,outputBatchCols], join='left') # update() method directly changes calling object\n",
    "        print(df_postBatch.match.value_counts(),\"\\n\",sep=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the aggregated postgeo file as a single csv (for inspection purposes only).\n",
    "file_path_census_postgeo_merged_file = file_path_census + \"\\\\postgeo_aggregated_total_file\\postGeoBatch_Total_\" + str(n)+ \"_Loops.csv\"  \n",
    "df_postBatch.to_csv(file_path_census_postgeo_merged_file, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      1      2 ... 337801 337802 337803] 337804 \n",
      "\n",
      "[     0      1      2 ... 337801 337802 337803] 337804 \n",
      "\n",
      "[     0      1      2 ... 337801 337802 337803] 337804 \n",
      "\n",
      "[     0      1      2 ... 337801 337802 337803] 337804 \n",
      "\n",
      "[     0      1      2 ... 337801 337802 337803] 337804 \n",
      "\n",
      "[True, True, True, True, True]\n",
      "df_preBatch_loop.index.max() == df_geo_merge_loop.index.max() :  True 337786 337786\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Check that list lengths and indexes of the original dataframe, prebatch dataframe, and postbatch dataframe are all the same.\n",
    "compareList = [df.index.values, df_preBatch.index.values, df_preBatch['Unique ID'].astype(int).values, \n",
    "               # df_geo_merge1.index.values, # Will only be the same on a total run. \n",
    "               df_postBatch.index.values, df_postBatch['Unique ID'].astype(int).values]  # loop dfs are filtered, so not included.\n",
    "for indexList in compareList:\n",
    "    print(indexList, len(indexList), \"\\n\")\n",
    "print([(ele == compareList[0]).all() for ele in compareList])\n",
    "print(\"df_preBatch_loop.index.max() == df_geo_merge_loop.index.max() : \", df_preBatch_loop.index.max() == df_geo_merge_loop.index.max(), df_preBatch_loop.index.max(), df_geo_merge_loop.index.max() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data successfully geocoded:  0.949\n",
      "True     320570\n",
      "False     17234\n",
      "Name: match, dtype: int64\n",
      "(337804, 87)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FullStreetAddress</th>\n",
       "      <th>City</th>\n",
       "      <th>StateOrProvince</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>County</th>\n",
       "      <th>SchoolDistrictName</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearBuiltEffective</th>\n",
       "      <th>ClosePrice</th>\n",
       "      <th>CloseDate</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalGarageAndParkingSpaces</th>\n",
       "      <th>TotalPhotos</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>PublicRemarks</th>\n",
       "      <th>Unique ID</th>\n",
       "      <th>address</th>\n",
       "      <th>match</th>\n",
       "      <th>tract</th>\n",
       "      <th>block</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11102 CROSS ROAD TRL</td>\n",
       "      <td>BRANDYWINE</td>\n",
       "      <td>MD</td>\n",
       "      <td>20613</td>\n",
       "      <td>PRINCE GEORGES</td>\n",
       "      <td></td>\n",
       "      <td>1951</td>\n",
       "      <td>0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>2017-11-22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>38.72173</td>\n",
       "      <td>-76.81473</td>\n",
       "      <td>SOLD \"AS IS\".  NO ACCESS TO THE HOUSE.  LEVEL ...</td>\n",
       "      <td>0</td>\n",
       "      <td>11102 CROSS ROAD TRL, BRANDYWINE, MD, 20613</td>\n",
       "      <td>True</td>\n",
       "      <td>801004</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4506 CEDELL PL</td>\n",
       "      <td>TEMPLE HILLS</td>\n",
       "      <td>MD</td>\n",
       "      <td>20748</td>\n",
       "      <td>PRINCE GEORGES</td>\n",
       "      <td>PRINCE GEORGE'S COUNTY PUBLIC SCHOOLS</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>309000.0</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>38.80514</td>\n",
       "      <td>-76.93090</td>\n",
       "      <td>Must See Home! 4 Bedroom 3 Full Bath Detached ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4506 CEDELL PL, TEMPLE HILLS, MD, 20748</td>\n",
       "      <td>True</td>\n",
       "      <td>801901</td>\n",
       "      <td>3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12607 WHITEHOLM DR</td>\n",
       "      <td>UPPER MARLBORO</td>\n",
       "      <td>MD</td>\n",
       "      <td>20774</td>\n",
       "      <td>PRINCE GEORGES</td>\n",
       "      <td>PRINCE GEORGE'S COUNTY PUBLIC SCHOOLS</td>\n",
       "      <td>1973</td>\n",
       "      <td>0</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>38.89964</td>\n",
       "      <td>-76.78679</td>\n",
       "      <td>Cash or FHA 203K loans only. Water is not avai...</td>\n",
       "      <td>2</td>\n",
       "      <td>12607 WHITEHOLM DR, UPPER MARLBORO, MD, 20774</td>\n",
       "      <td>True</td>\n",
       "      <td>800521</td>\n",
       "      <td>3003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FullStreetAddress            City StateOrProvince  PostalCode  \\\n",
       "0  11102 CROSS ROAD TRL      BRANDYWINE              MD       20613   \n",
       "1        4506 CEDELL PL    TEMPLE HILLS              MD       20748   \n",
       "2    12607 WHITEHOLM DR  UPPER MARLBORO              MD       20774   \n",
       "\n",
       "           County                     SchoolDistrictName  YearBuilt  \\\n",
       "0  PRINCE GEORGES                                              1951   \n",
       "1  PRINCE GEORGES  PRINCE GEORGE'S COUNTY PUBLIC SCHOOLS       1965   \n",
       "2  PRINCE GEORGES  PRINCE GEORGE'S COUNTY PUBLIC SCHOOLS       1973   \n",
       "\n",
       "   YearBuiltEffective  ClosePrice   CloseDate  ...  \\\n",
       "0                   0     60000.0  2017-11-22  ...   \n",
       "1                   0    309000.0  2017-10-15  ...   \n",
       "2                   0    245000.0  2018-08-24  ...   \n",
       "\n",
       "   TotalGarageAndParkingSpaces TotalPhotos  Latitude Longitude  \\\n",
       "0                            0           7  38.72173 -76.81473   \n",
       "1                            1          14  38.80514 -76.93090   \n",
       "2                            1          14  38.89964 -76.78679   \n",
       "\n",
       "                                       PublicRemarks  Unique ID  \\\n",
       "0  SOLD \"AS IS\".  NO ACCESS TO THE HOUSE.  LEVEL ...          0   \n",
       "1  Must See Home! 4 Bedroom 3 Full Bath Detached ...          1   \n",
       "2  Cash or FHA 203K loans only. Water is not avai...          2   \n",
       "\n",
       "                                         address  match   tract block  \n",
       "0    11102 CROSS ROAD TRL, BRANDYWINE, MD, 20613   True  801004  1005  \n",
       "1        4506 CEDELL PL, TEMPLE HILLS, MD, 20748   True  801901  3005  \n",
       "2  12607 WHITEHOLM DR, UPPER MARLBORO, MD, 20774   True  800521  3003  \n",
       "\n",
       "[3 rows x 87 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 7: Reverse column name changes and save as MarylandData_postGeo_orig2.csv. Set postBatch_output as the new df for inspection purposes.\n",
    "df_postBatch_output = df_postBatch.rename(columns = batchColumns_inverseDict).copy()\n",
    "\n",
    "print(\"Proportion of data successfully geocoded: \",round(df_postBatch_output.match.value_counts()[0]/(df_postBatch_output.match.value_counts()[0]+df_postBatch_output.match.value_counts()[1]),3))\n",
    "print(df_postBatch_output.match.value_counts())\n",
    "print(df_postBatch_output.shape)\n",
    "df_postBatch_output.head(3)\n",
    "\n",
    "# Save file as \"..._orig2\" and print the number of hours the entire script has taken up until this point. \n",
    "df_postBatch_output.to_csv(file_path_census + \"MarylandData_postGeo_orig2.csv\",index=False) \n",
    "print(\"--- %s hours ---\" % round((time.time() - start_time)/3600,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 26.81 hours ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_19132\\4092739351.py:2: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv(file_path_census + \"MarylandData_postGeo_orig3.csv\", keep_default_na=False, dtype={'Levels':'object'})\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Load the previously saved \"_orig2\" file.\n",
    "df2 = pd.read_csv(file_path_census + \"MarylandData_postGeo_orig2.csv\", keep_default_na=False, dtype={'Levels':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining rows left to censusGeocode: 5\n",
      "Success 20 37-L RIDGE RD #L 39.00335544 -76.8726714\n",
      "Success 30 17401 CENTRAL AVE 38.89939899 -76.70377014\n",
      "Success 31 16-K RIDGE RD #K 38.99843485 -76.87813466\n",
      "Success 42 11250 KETTERING PL 38.89386 -76.80909\n",
      "Success 91 9702 TAM O SHANTER DR 38.75726 -76.8015\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 1 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 2 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 3 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 4 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 5 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 6 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 7 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 8 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 9 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 10 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 11 \n",
      "\n",
      "lac and lon that are both not NANs: 100\n",
      "Tracts that are NANs: 0\n",
      "End Loop: 12 \n",
      "\n",
      "CPU times: total: 266 ms\n",
      "Wall time: 3.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 9: Identify rows with valid Lat/Long and loop until each row has an accompaning 'tract' value.\n",
    "count = 0\n",
    "latlonMask = (df2[['Latitude', 'Longitude']].notna().all('columns')&(df2['Latitude']!=0.0)&(df2['Longitude']!=0.0))\n",
    "tractMask = (df2['tract']=='')\n",
    "fullMask = latlonMask & (tractMask)\n",
    "\n",
    "print(\"Total remaining rows left to censusGeocode:\",len(df2[fullMask]) )\n",
    "while (len(df2[fullMask]) >= 0) and (count < 12):  # count<10\n",
    "    df_geoTemp = df2[fullMask].copy()\n",
    "    df_geoTemp = getTract_LatLong(df_geoTemp)\n",
    "    df2[fullMask] = df_geoTemp\n",
    "\n",
    "    count += 1\n",
    "    latlonMask = (df2[['Latitude', 'Longitude']].notna().all('columns')&(df2['Latitude']!=0.0)&(df2['Longitude']!=0.0))\n",
    "    tractMask = (df2['tract']==\"\")\n",
    "    fullMask = latlonMask & (tractMask)\n",
    "    print(\"lac and lon that are both not NANs:\", len(df2[latlonMask]))\n",
    "    print(\"Tracts that are NANs:\", len(df2[tractMask]))\n",
    "    print(\"End Loop:\", count, \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.15 hours ---\n",
      "CPU times: total: 12.9 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 10: Save the final CSV and print the insane amount of time the script took to run. \n",
    "save_big_df_as_small_csvs(df2, csv_output_path = \"geocoded_real_estate_files//\", base_name = \"MarylandData_postGeo_orig3\", row_max = 40000)\n",
    "print(\"--- %s hours ---\" % round((time.time() - start_time)/3600,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRASH CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5c04299b84f0fefe31efd96fe46968254e5059f533561286d635df025d161a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
